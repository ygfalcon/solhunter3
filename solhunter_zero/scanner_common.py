# solhunter_zero/scanner_common.py
from __future__ import annotations

import asyncio
import logging
import os
from pathlib import Path
from typing import Any, Dict, Iterable, List
from urllib.parse import urlparse, urlunparse

import aiohttp

from .env_settings import api_key, api_url, env_value
from .lru import TTLCache

logger = logging.getLogger(__name__)

TREND_CACHE_TTL = float(os.getenv("TREND_CACHE_TTL", "45") or 45.0)
TREND_CACHE: TTLCache = TTLCache(maxsize=1, ttl=TREND_CACHE_TTL)
_TREND_CACHE_KEY = "trending_tokens"


def _derive_ws_from_rpc(rpc_url: str | None) -> str | None:
    if not rpc_url:
        return None
    parsed = urlparse(rpc_url)
    if not parsed.scheme:
        return None
    if parsed.scheme.startswith("ws"):
        return rpc_url
    if parsed.scheme == "https":
        scheme = "wss"
    elif parsed.scheme == "http":
        scheme = "ws"
    else:
        return None
    return urlunparse(parsed._replace(scheme=scheme))


def _ensure_env(name: str, value: str) -> str:
    os.environ[name] = value
    return value


def refresh_runtime_values() -> None:
    """Synchronise RPC/WS/BirdEye globals with current environment."""
    global SOLANA_RPC_URL, SOLANA_WS_URL
    global JUPITER_WS_URL, PHOENIX_WS_URL, METEORA_WS_URL
    global BIRDEYE_API_KEY, HEADERS

    rpc = env_value("SOLANA_RPC_URL", strip=True)
    SOLANA_RPC_URL = _ensure_env("SOLANA_RPC_URL", rpc)

    derived_ws = _derive_ws_from_rpc(SOLANA_RPC_URL)
    ws = env_value("SOLANA_WS_URL", default=derived_ws or "", strip=True)
    SOLANA_WS_URL = _ensure_env("SOLANA_WS_URL", ws or derived_ws or "")

    JUPITER_WS_URL = env_value("JUPITER_WS_URL", default=SOLANA_WS_URL, strip=True)
    PHOENIX_WS_URL = env_value("PHOENIX_WS_URL", default=SOLANA_WS_URL, strip=True)
    METEORA_WS_URL = env_value("METEORA_WS_URL", default=SOLANA_WS_URL, strip=True)

    BIRDEYE_API_KEY = api_key("BIRDEYE_API_KEY")

    HEADERS = {
        "accept": "application/json",
        "X-API-KEY": BIRDEYE_API_KEY,
        "x-chain": os.getenv("BIRDEYE_CHAIN", "solana"),
    }


def get_solana_ws_url() -> str:
    return SOLANA_WS_URL

# ---------------------------------------------------------------------
# BirdEye config â€“ environment configured
# ---------------------------------------------------------------------
BIRDEYE_API: str = api_url("BIRDEYE_API_URL") or "https://public-api.birdeye.so"

refresh_runtime_values()

# ---------------------------------------------------------------------
# Misc exports (safe stubs for imports)
# ---------------------------------------------------------------------

TOKEN_SUFFIX = os.getenv("TOKEN_SUFFIX", "token")
TOKEN_KEYWORDS = [k.strip() for k in os.getenv("TOKEN_KEYWORDS", "token,coin").split(",") if k.strip()]

OFFLINE_TOKENS: List[str] = [
    "So11111111111111111111111111111111111111112",  # Wrapped SOL
    "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",  # USDC
]

def token_matches(token_info: Dict[str, Any] | str, patterns: Iterable[str] | None = None) -> bool:
    if patterns is None:
        return True
    if isinstance(token_info, str):
        return True
    sym = str(token_info.get("symbol", "")).lower()
    name = str(token_info.get("name", "")).lower()
    for p in patterns:
        q = str(p).lower()
        if q and (q in sym or q in name):
            return True
    return False

async def fetch_trending_tokens_async(limit: int = 50) -> List[str]:
    limit = max(1, int(limit))

    async def _fetch_remote() -> List[str]:
        url = os.getenv("TREND_CACHE_URL", f"{BIRDEYE_API}/defi/trending")
        timeout = float(os.getenv("TREND_CACHE_TIMEOUT", "10") or 10.0)
        session = aiohttp.ClientSession()
        try:
            async with session.get(url, timeout=timeout) as resp:
                resp.raise_for_status()
                payload = await resp.json()
        finally:
            close = getattr(session, "close", None)
            if close:
                if asyncio.iscoroutinefunction(close):
                    await close()
                else:
                    close()
        items: List[Any] = []
        if isinstance(payload, dict):
            for key in ("trending", "items", "tokens", "data", "results"):
                candidate = payload.get(key)
                if isinstance(candidate, list):
                    items = candidate
                    break
            else:
                data = payload.get("data")
                if isinstance(data, list):
                    items = data
        elif isinstance(payload, list):
            items = payload
        results: List[str] = []
        for entry in items:
            if isinstance(entry, dict):
                address = (
                    entry.get("address")
                    or entry.get("mint")
                    or entry.get("mintAddress")
                    or entry.get("tokenAddress")
                )
            else:
                address = entry
            if isinstance(address, str) and address:
                results.append(address)
            if len(results) >= limit:
                break
        return results

    try:
        tokens = await TREND_CACHE.get_or_set_async(_TREND_CACHE_KEY, _fetch_remote)
    except Exception as exc:  # pragma: no cover - defensive
        logger.debug("Trending token cache fetch failed: %s", exc)
        return []

    if not isinstance(tokens, list):
        return []

    return list(tokens)[:limit]


def scan_tokens_from_file(path: str | os.PathLike | None, *, limit: int | None = None) -> List[str]:
    if not path:
        return []
    file_path = Path(path).expanduser()
    if not file_path.exists():
        return []
    tokens: List[str] = []
    for line in file_path.read_text(encoding="utf-8").splitlines():
        mint = line.strip()
        if not mint:
            continue
        tokens.append(mint)
        if limit is not None and len(tokens) >= limit:
            return tokens[:limit]
    return tokens


def scan_tokens_from_directory(directory: str | os.PathLike, *, limit: int | None = None) -> List[str]:
    base = Path(directory).expanduser()
    if not base.exists() or not base.is_dir():
        return []
    tokens: List[str] = []
    for child in sorted(base.iterdir()):
        if not child.is_file():
            continue
        tokens.extend(scan_tokens_from_file(child, limit=limit))
        if limit is not None and len(tokens) >= limit:
            return tokens[:limit]
    return tokens
